{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3a2f3b",
   "metadata": {},
   "source": [
    "## Challenge TelecomX-2-ML – Análisis Predictivo de Churn\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "En esta segunda etapa del Challenge de Data Science de Alura, el enfoque se centra en el **modelado predictivo** para anticipar la evasión de clientes (Churn) en TelecomX.\n",
    "Luego de completar el proceso de **ETL y Análisis Exploratorio (EDA)**, el objetivo ahora es construir, evaluar y optimizar modelos de Machine Learning que permitan:\n",
    "\n",
    "* Predecir qué clientes tienen mayor probabilidad de abandonar el servicio.\n",
    "* Identificar las variables más influyentes en la decisión de churn.\n",
    "* Proporcionar recomendaciones estratégicas basadas en evidencia predictiva.\n",
    "\n",
    "\n",
    "Flujo de implementación:\n",
    "\n",
    "1. Feature Engineering\n",
    "2. Preprocesamiento\n",
    "3. Train/Test SplitPipeline\n",
    "4. Cross-Validation (en Train)\n",
    "5. Comparación de modelos\n",
    "6. Optimización\n",
    "7. Entrenamiento final\n",
    "8. Evaluación en Test\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cedbcbf",
   "metadata": {},
   "source": [
    "## Implementación del Modelado Predictivo\n",
    "\n",
    "### 1 Ingeniería de Características (Feature Engineering)\n",
    "\n",
    "En esta fase se optimizan y crean variables para mejorar la capacidad predictiva del modelo.\n",
    "\n",
    "* **Creación de `num_services`**: Variable que cuantifica la intensidad de uso del cliente. Un mayor número de servicios puede reflejar mayor fidelización o mayor complejidad de abandono.\n",
    "* **Variables derivadas**: Posible creación de ratios como gasto promedio por servicio.\n",
    "* **Eliminación de variables irrelevantes o redundantes**:\n",
    "\n",
    "  * `CustomerID` (identificador sin valor predictivo).\n",
    "  * Análisis de variables altamente correlacionadas que puedan generar multicolinealidad.\n",
    "* **Análisis de importancia preliminar** para validar aporte de nuevas variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 Preprocesamiento de Datos (Data Preprocessing)\n",
    "\n",
    "Preparación técnica de los datos para su uso en modelos de Machine Learning.\n",
    "\n",
    "* **Codificación de Variables Categóricas**:\n",
    "\n",
    "  * One-Hot Encoding para variables nominales (ej. `PaymentMethod`).\n",
    "  * Label Encoding para la variable objetivo (`Churn`: 0 = No, 1 = Sí).\n",
    "\n",
    "* **Escalado de Variables Numéricas**:\n",
    "\n",
    "  * Estandarización (StandardScaler) o Normalización (MinMaxScaler).\n",
    "  * Aplicado a variables como:\n",
    "\n",
    "    * `Tenure`\n",
    "    * `ChargesMonthly`\n",
    "    * `num_services`\n",
    "\n",
    "* **Pipeline de Preprocesamiento**:\n",
    "  Implementación de `Pipeline` para evitar data leakage y garantizar reproducibilidad.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 División del Dataset\n",
    "\n",
    "Separación estratégica para validar el desempeño real del modelo.\n",
    "\n",
    "* **Definición de variables**:\n",
    "\n",
    "  * `X` → Variables predictoras.\n",
    "  * `y` → Variable objetivo (Churn).\n",
    "\n",
    "* **Train/Test Split**:\n",
    "\n",
    "  * 80% entrenamiento\n",
    "  * 20% prueba\n",
    "  * Uso de `stratify=y` para mantener proporción de churn.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 Validación Cruzada (Cross-Validation)\n",
    "\n",
    "Antes de evaluar el modelo final, se implementa validación cruzada:\n",
    "\n",
    "* **K-Fold Cross Validation (k=5 o 10)**\n",
    "  Permite:\n",
    "\n",
    "  * Reducir varianza en la estimación del rendimiento.\n",
    "  * Detectar sobreajuste.\n",
    "  * Obtener métricas promedio más robustas.\n",
    "\n",
    "Esta etapa es clave para asegurar estabilidad del modelo, se aplica sólo alconjunto de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 5 Selección y Entrenamiento de Modelos\n",
    "\n",
    "Se exploran diferentes algoritmos para comparar desempeño:\n",
    "\n",
    "* **Regresión Logística**\n",
    "\n",
    "  * Modelo base interpretable.\n",
    "  * Buena referencia inicial.\n",
    "\n",
    "* **Random Forest**\n",
    "\n",
    "  * Maneja relaciones no lineales.\n",
    "  * Robusto ante outliers y ruido.\n",
    "\n",
    "* **Gradient Boosting (ej. XGBoost o similares)**\n",
    "\n",
    "  * Alto poder predictivo.\n",
    "  * Captura patrones complejos.\n",
    "\n",
    "* **LightGBM**\n",
    "\n",
    "  * Más rápido y eficiente que otros boosting\n",
    "\n",
    "  * Excelente rendimiento en datasets estructurados\n",
    "\n",
    "  * Maneja bien variables categóricas codificadas\n",
    "\n",
    "\n",
    "Cada modelo se entrena utilizando validación cruzada para comparación objetiva.\n",
    "\n",
    "---\n",
    "\n",
    "### 6 Evaluación de Métricas\n",
    "\n",
    "Dado que el problema es de clasificación binaria con posible desbalance, se priorizan las siguientes métricas:\n",
    "\n",
    "* **Matriz de Confusión**\n",
    "\n",
    "  * Falsos Positivos (FP)\n",
    "  * Falsos Negativos (FN)\n",
    "\n",
    "* **Recall (Sensibilidad)**\n",
    "\n",
    "  * Métrica crítica en churn: identificar correctamente a quienes se irán.\n",
    "\n",
    "* **Precision**\n",
    "\n",
    "  * Minimizar falsas alarmas.\n",
    "\n",
    "* **F1-Score**\n",
    "\n",
    "  * Balance entre Precision y Recall.\n",
    "\n",
    "* **ROC-AUC**\n",
    "\n",
    "  * Evaluación global del poder discriminatorio del modelo.\n",
    "\n",
    "- En `variables` como churn, **Recall** tiene mayor relevancia en resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 Optimización de Hiperparámetros\n",
    "\n",
    "Para maximizar el rendimiento:\n",
    "\n",
    "* **GridSearchCV**\n",
    "* **RandomizedSearchCV**\n",
    "\n",
    "Aplicado sobre el modelo con mejor desempeño preliminar.\n",
    "\n",
    "Objetivo:\n",
    "\n",
    "* Ajustar profundidad de árboles.\n",
    "* Número de estimadores.\n",
    "* Regularización.\n",
    "* Learning rate (en boosting).\n",
    "\n",
    "Todo validado mediante Cross-Validation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8 Modelo Final y Análisis de Importancia\n",
    "\n",
    "* Entrenamiento final con mejores hiperparámetros.\n",
    "* Evaluación sobre conjunto de prueba.\n",
    "* Análisis de importancia de variables.\n",
    "* Interpretación estratégica de resultados.\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados Esperados\n",
    "\n",
    "* Modelo robusto y validado.\n",
    "* Identificación de variables clave en churn.\n",
    "* Segmentación de clientes en riesgo.\n",
    "* Base técnica para implementar estrategias de retención.\n",
    "\n",
    "---\n",
    "\n",
    "## Impacto Estratégico\n",
    "\n",
    "Este modelo predictivo permite:\n",
    "\n",
    "* Implementar campañas de retención dirigidas.\n",
    "* Optimizar recursos comerciales.\n",
    "* Reducir tasa de churn.\n",
    "* Incrementar el Customer Lifetime Value (CLV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# ==============================\n",
    "\n",
    "# Ignorar warnings irrelevantes\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# BIBLIOTECAS PARA MANEJO DE DATOS\n",
    "# ==============================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# BIBLIOTECAS PARA PREPROCESAMIENTO - ML\n",
    "# ==============================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ==============================\n",
    "# BIBLIOTECAS PARA VISUALIZACIÓN\n",
    "# ==============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURACIÓN DE VISUALIZACIONES\n",
    "# ==============================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Si trabajas en Jupyter, puedes activar esto:\n",
    "# %matplotlib inline\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURACIÓN DE PANDAS\n",
    "# ==============================\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# ==============================\n",
    "# PALETA DE COLORES PERSONALIZADA\n",
    "# ==============================\n",
    "\n",
    "color_no_churn = '#2C3E50'  # Azul oscuro elegante\n",
    "color_churn = '#E74C3C'     # Rojo elegante\n",
    "\n",
    "color_palette = [\n",
    "  '#F8F9FA', # Fondo Limpio (Ghost White)\n",
    "  '#E9ECEF', # Bordes/Secciones (Slate Gray)\n",
    "  '#22577A', # Títulos/Estructura (Dark Imperial Blue)\n",
    "  '#38A3A5', # Procesos activos (Cadet Blue)\n",
    "  '#57CC99', # Éxito/Validación (Emerald)\n",
    "  '#80ED99'  # Acentos ligeros (Light Green)\n",
    "]\n",
    "\n",
    "sns.set_palette(color_palette)\n",
    "\n",
    "# ==============================\n",
    "# CREAR CARPETA PARA IMÁGENES\n",
    "# ==============================\n",
    "\n",
    "os.makedirs('imgs', exist_ok=True)\n",
    "\n",
    "print('✅ Bibliotecas importadas y entorno configurado correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fab318",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Previo análisis con dataset generado en la 1ra etapa y al no encontrar mayores inconsistencias, se utiliza el dataset proprocionado por Alura One \"dataset_tratado.csv\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/LenninTemoche/Challenge-TelecomX-2-Alura-One-DS/refs/heads/main/datos_tratados.csv\"\n",
    "\n",
    "df = pd.read_csv(url, sep=\",\")\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c758959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definimos las columnas que representan servicios\n",
    "service_cols = [\n",
    "    'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
    "]\n",
    "\n",
    "# 2. Función para contar los servicios activos de cada cliente\n",
    "def count_services(row):\n",
    "    count = 0\n",
    "    # InternetService: Si es 'DSL' o 'Fiber optic' cuenta como 1 (si es 'No', es 0)\n",
    "    if row['InternetService'] != 'No':\n",
    "        count += 1\n",
    "    \n",
    "    # Para el resto, sumamos 1 si el valor es 'Yes'\n",
    "    for col in service_cols:\n",
    "        if col != 'InternetService' and row[col] == 'Yes':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Aplicamos la función para crear la nueva columna\n",
    "df['num_services'] = df.apply(count_services, axis=1)\n",
    "\n",
    "print(f\"✅ Nueva columna creada: 'num_services' (Rango: {df['num_services'].min()} - {df['num_services'].max()} servicios)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d4a05",
   "metadata": {},
   "source": [
    "# 1. Definimos las columnas que representan servicios\n",
    "service_cols = [\n",
    "    'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
    "]\n",
    "\n",
    "# 2. Función para contar los servicios activos de cada cliente\n",
    "def count_services(row):\n",
    "    count = 0\n",
    "    # InternetService: Si es 'DSL' o 'Fiber optic' cuenta como 1 (si es 'No', es 0)\n",
    "    if row['InternetService'] != 'No':\n",
    "        count += 1\n",
    "    \n",
    "    # Para el resto, sumamos 1 si el valor es 'Yes'\n",
    "    for col in service_cols:\n",
    "        if col != 'InternetService' and row[col] == 'Yes':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Aplicamos la función para crear la nueva columna\n",
    "df['num_services'] = df.apply(count_services, axis=1)\n",
    "\n",
    "print(f\"✅ Nueva columna creada: 'num_services' (Rango: {df['num_services'].min()} - {df['num_services'].max()} servicios)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
